---
title: "Week 8: Justice and Representation in Information Systems"
subtitle: "IS 505 Information Organization and Access"
author: 
  - Dr. Manika Lamba
  - Dr. Liliana Giusti Serra
date: '5 March 2024'
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: images/uiuc.png
    css: styles.css
---

## Class Calendar

![](images/calendar.png){fig-align="center"}

##  {.smaller}

::: columns
::: {.column width="50%"}
**`Group A-K (April 2)`**

1.  Avouris, Cassiani

2.  Barrett, Abigail

3.  Boyle, Nora

4.  Brown, Alyssa

5.  Carrithers, Marguerite

6.  Ding, Weiyu

7.  Eggimann, Kelsey

8.  Faro, Anika

9.  Good, Amanda

10. Harper, Evalyn

11. Harrington, KeSean

12. Kedzierski, Haley

13. Kerr-Dennhardt, Delia
:::

::: {.column width="50%"}
**`Group L-S (April 9)`**

1.  Lawless, Jessie
2.  Mason, Chloe
3.  Mullins, Logan
4.  Nagel, Nicole
5.  Parks, Ren
6.  Pihlstrom, Katherine
7.  Quintana, Kari
8.  Ryan, Isabel
9.  Shepherd, Haley
10. Smerz, Allyson
11. Steiner, Madeline
12. Streeter, Olivia
:::
:::

## Today's Class

-   Introduction
-   Lecture
-   Break (10 minutes)
-   Discussion
-   Break (10 minutes)
-   Class activity

## Week 8 Readings

![](images/readings.png){fig-align="center"}

## Digital Trace Data {.smaller}

::: columns
::: {.column width="70%"}
**`Ten common characteristics of big data`**

-   **Big** - *large datasets are a means to an end; they are not an end in themselves*

::: {style="font-size: 60%;"}
*\[Our\] corpus contains over 500 billion words, in English (361 billion), French (45 billion), Spanish (45 billion), German (37 billion), Chinese (13 billion), Russian (35 billion), and Hebrew (2 billion) [(Michel et al., 2011)](https://doi.org/10.1126/science.1199644)*
:::

-   **Always-on** - *helps to identify unexpected events and real-time measurement*

::: {style="font-size: 60%;"}
[*Ceren Budak and Duncan Watts (2015)*](https://doi.org/10.15195/v2.a18) *were able to do more by using the always-on nature of Twitter to study protesters who used Twitter before, during, and after the event. And, they were able to create a comparison group of nonparticipants before, during, and after the event*
:::

-   **Nonreactive** - *Measurement in big data sources is much less likely to change behavior*

::: {style="font-size: 60%;"}
*even though some big data sources are nonreactive, they are not always free of social desirability bias, the tendency for people to want to present themselves in the best possible way*
:::

::: {style="font-size: 60%;"}
:::
:::

::: {.column width="30%"}
[![](images/cover.jpeg){fig-alt="https://www.bitbybitbook.com/en/1st-ed/preface/" fig-align="center"}](https://www.bitbybitbook.com/en/1st-ed/preface/)

First Published in 2019
:::
:::

## Digital Trace Data {.smaller}

-   [**Incomplete**]{style="color: blue;"} - *No matter how big your big data, it probably doesn’t have the information you want*

::: {style="font-size: 60%;"}
*big data tends to be missing three types of information useful for social research: demographic information about participants, behavior on other platforms, and data to operationalize theoretical constructs*
:::

-   [**Inaccessible**]{style="color: blue;"} - *Data held by companies and governments are difficult for researchers to access*

-   [**Non-representative**]{style="color: blue;"} - *Non-representative data are bad for out-of-sample generalizations, but can be quite useful for within-sample comparisons*

-   **Drifting** - *Population drift, usage drift, and system drift make it hard to use big data sources to study long-term trends*

-   [**Algorithmically confounded**]{style="color: blue;"} - [*Behavior in big data systems is not natural; it is driven by the engineering goals of the systems*]{style="color: orange;"}

Algorithmic confounding means that we should be cautious about any claim regarding human behavior that comes from a single digital system, no matter how big

## Digital Trace Data {.smaller}

-   [**Dirty**]{style="color: blue;"} - *Big data sources can be loaded with junk and spam*

-   [**Sensitive**]{style="color: blue;"} - *Some of the information that companies and governments have is sensitive*

    -   It turns out to be quite tricky to decide what information is actually sensitive.

    -   In 2006 Netflix released 100 million movie ratings provided by almost 500,000 members and had an open call where people from all over the world submitted algorithms that could improve Netflix’s ability to recommend movies.

    -   Before releasing the data, Netflix removed any obvious personally identifying information, such as names.

    -   But, just two weeks after the data was released Arvind Narayanan and Vitaly Shmatikov (2008) showed that it was possible to learn about specific people’s movie ratings.

    -   Even though an attacker could discover a person’s movie ratings, there still doesn’t seem to be anything sensitive here.

    -   While that might be true in general, for at least some of the 500,000 people in the dataset, movie ratings were sensitive.

## Digital Trace Data {.smaller}

-   [**Sensitive**]{style="color: blue;"}

    -   In fact, in response to the release and re-identification of the data, a closeted lesbian woman joined a class-action suit against Netflix.

Here’s how the problem was expressed in this lawsuit

> “\[M\]ovie and rating data contains information of a … highly personal and sensitive nature. The member’s movie data exposes a Netflix member’s personal interest and/or struggles with various highly personal issues, including sexuality, mental illness, recovery from alcoholism, and victimization from incest, physical abuse, domestic violence, adultery, and rape.”

## Algorithmic Confounding {.smaller}

> It occurs when a computer system reflects the implicit values of the humans who are involved in collecting, selecting, or using data

![](images/bias.jpeg){fig-align="center"}

::: footer
Source: <https://www.weforum.org/agenda/2021/07/ai-machine-learning-bias-discrimination/>
:::

## When Google Got Flu Wrong {.smaller}

::: incremental
-   In 2008 Google and CDC launched a new interface called Google Flu Trends to [monitor influenza-like illness (ILI)]{style="color: blue;"}

-   It relied on data mining records of [flu-related search terms entered in Google’s search engine]{style="color: blue;"}, combined with computer modelling

-   It estimated almost exactly matched the CDC’s own surveillance data over time — and it delivers them several days faster than the CDC can

-   In 2013, [US flu season]{style="color: blue;"} seems to have [confounded its algorithms]{style="color: blue;"}

-   It estimated for the Christmas national peak of flu is almost double the CDC’s (see ‘Fever peaks’), and some of its state data show even larger discrepancies

-   It was not the first time that a flu season has tripped Google up

-   In 2009, Flu Trends had to tweak its algorithms after its models badly underestimated ILI in the United States at the start of the H1N1 (swine flu) pandemic — a glitch attributed to changes in people’s search behaviour as a result of the exceptional nature of the pandemic
:::

::: footer
Source: <https://www.nature.com/articles/494155a>
:::

## When Google Got Flu Wrong {.smaller}

![](images/flu.png){fig-align="center"}

::: footer
Source: <https://www.nature.com/articles/494155a>
:::

## Algorithmic Biasness {.smaller}

![](images/bias.png){fig-align="center"}

::: footer
Lamba, M., Madhusudhan, M. (2022). Text Data and Mining Ethics. In: Text Mining for Information Professionals. Springer, Cham. https://doi.org/10.1007/978-3-030-85085-2_11

Read more here: <https://cte.ku.edu/addressing-bias-ai>
:::

## Algorithmic Biasness (Cont.) {.smaller}

-   Algorithms might disseminate social biases against certain groups of sociodemographic factors (such as race, gender, geography)
-   The output of these algorithms is primarily dependent on the [annotated datasets and is sensitive to social bias created by humans]{style="color: blue;"}
-   An algorithm that uses [both text and metadata to learn is likely to be highly biased]{style="color: blue;"} as metadata consists of the author’s nationality, discipline, etc., when compared to an algorithm with text-only data
-   Even with text-only data, algorithms will still learn bias due to the language problems generated by [second-order effects]{style="color: blue;"} for text-based machine learning
-   Additionally, when using chatbots (*such as ChatGPT*) to provide realtime recommendations, the dialogue of chatbot can be modelled with available metadata to adjust the features of the replier in terms of gender, age, and mood *`(Metaphors in HCI)`*

::: footer
Lamba, M., Madhusudhan, M. (2022). Text Data and Mining Ethics. In: Text Mining for Information Professionals. Springer, Cham. https://doi.org/10.1007/978-3-030-85085-2_11
:::

## Ways to Mitigate Biases {.smaller}

-   Understanding how the data was generated
-   Using tools that identify bias in models and algorithms such as `FairML`, `IBM AI Fairness 360`, `Accenture’s “Teach and Test” Methodology,` `Google’s What-If Tool`, and `Microsoft’s Fairlearn`
-   Making the data, process, and outcome open, thus making it transparent and helping us to judge
-   Creating algorithms and standards that can be adapted from one application to another
-   Following the set of standards proposed by the Association for Computing Machinery US Public Policy Council and applying them at every stage in the algorithm creation process
-   Enforcing accountability in policies during [*auditing in pre-and post-processing as well as standardized assessment*]{.underline} as algorithms do not make mistakes, but humans do

::: footer
Lamba, M., Madhusudhan, M. (2022). Text Data and Mining Ethics. In: Text Mining for Information Professionals. Springer, Cham. https://doi.org/10.1007/978-3-030-85085-2_11
:::

## Further Interesting Readings {.smaller}

::: r-stack
![](images/new.jpeg){.fragment width="250" height="350"} ![](images/new2.jpeg){.fragment width="250" height="350"} ![](images/new3.jpeg){.fragment width="250" height="350"} ![](images/new4.jpeg){.fragment width="250" height="350"}
:::

## Biases in LoC Subject Headings {.smaller}

-   Language vocabularies do not include every langauge
-   LOC does not represent marginalized groups of people
-   Descriptive practices have been based on systems and standards ingrained with white supremacy, misogyny, and homophobia
-   Much of this descriptive work contains insensitive, outdated, or inappropriate language that reflects the harmful biases built into descriptive systems
-   From POV - *white, male, christian, heterosexual, cisgender*
-   Historical bias, unconscious bias, combination of purposeful and unpurposeful bias
-   Homosaurus term

![](images/loc9.png){fig-align="center"}

::: footer
[More Info](https://www.youtube.com/watch?v=X0PVYgwHhVo)
:::

## Bias Hiding in Your Library {.smaller}

`The ‘straight white American man’ assumption`

*Without gender, race or geographic qualifications, “Astronauts” can be assumed to mean white American men in terms of library subjects*

![Official Library of Congress subject headings involving astronaut. Amanda Ros. CC BY](images/loc.png "Official Library of Congress subject headings involving astronauts."){fig-alt="Official Library of Congress subject headings involving astronauts." fig-align="center"}

::: footer
<https://theconversation.com/the-bias-hiding-in-your-library-111951>
:::

## Bias Hiding in Your Library {.smaller}

-   Nurses were divided equitably for both Male and Female

-   Under Prostitutes, there was only “Male prostitute” SH, revealing the generic assumption that most prostitutes are female

![Official Library of Congress subject headings for three professions traditionally perceived as female. Amanda Ros. CC BY](images/loc2.png){fig-align="center"}

::: footer
<https://theconversation.com/the-bias-hiding-in-your-library-111951>
:::

## Beginning of Change

![](images/ct.png){fig-align="center"}

## Critical Theory

-   Feminist Theory; Social Influences - Hope Olson (1990s-present)
-   Critical Race Theory - Jonathan Furner (2000s)
-   "Three Decades Since *Prejudice and Antipathies*: A Study of Changes in the Library of Congress Subject Headings" - Steven Knowlton (2005)
-   [Queer Theory](https://academicworks.cuny.edu/gc_pubs/577/#:~:text=Queer%20theory%20invites%20a%20shift,just%20as%20critical%20catalogers%20do) - Emily Drabinski (2010s) \[*Currently President of the American Library Association (ALA)*\]

:::footer
[Engaging an Author in a Critical Reading of Subject Headings](https://doi.org/10.24242/jclis.v1i1.20)

[Advancing the Relationship between Critical
Cataloging and Critical Race Theory](https://doi.org/10.1080/01639374.2022.2089936)
:::

## Current State

Main issues today:

-   LQBTIQA+
-   Gender
-   Racial categories
    -   races have been named by dominant group (white)
-   Indigenous People
    -   broad and inaccurate subject term
-   Western bias

## Some Positive Changes in LOC SH {.smaller}

::: {style="font-size: 91%;"}
[**Ethnicity**]{.underline}

-   In late 1970s, “`Afro-Americans`” replaced *“Negroes”*

-   This was in turn replaced by “`African Americans`” or “`Blacks`” in 2000

[**Medical Condition**]{.underline}

-   In 2001, “`People with mental disabilities`” replaced “*Mentally handicapped”* and *“Retarded persons”*

[**Gender**]{.underline}

-   Gender identity is also an area where positive changes have been made

-   LGBT subjects have been distinguished and classed under `“Sexual minorities”` since 1972, rather than being under the subject *“Sexual deviations”*. *“Sexual deviations”* does not even exist as a subject heading anymore

-   In December, the Library of Congress changed the broader term from *“sexual minorities”* to simply “`persons`"
:::

::: footer
<https://theconversation.com/the-bias-hiding-in-your-library-111951>
:::

## Efforts by Library of Congress {.smaller}

![](images/loc3.png){fig-align="center" width="500"}

::: footer
<https://www.loc.gov/aba/cataloging/subject/indigenous.html#in-process>
:::

## Efforts by Library of Congress {.smaller}

![2023-03-27](images/loc6.png "2023-03-27"){fig-alt="2023-03-27" fig-align="center"}

::: footer
<https://www.loc.gov/aba/cataloging/subject/Evaluation-Headings-Indigenous-Peoples.pdf>
:::

## Efforts by Library of Congress {.smaller}

![](images/loc4.png){fig-align="center"}

::: footer
<https://www.loc.gov/aba/pcc/saco/cpsoed/psd-160321.html>
:::

## Efforts by Library of Congress {.smaller}

![](images/loc5.png){fig-align="center"}

::: footer
<https://www.loc.gov/aba/cataloging/subject/lcsh-process.html>
:::

## Efforts by Libraries {.smaller}

> Libraries are making efforts to redress this problematic history

`Statement on Harmful Language in Cataloging and Archival Description by University of Virginia Library`

-   They are actively removing the harmful language in their legacy records

-   They acknowledge that many LCSH headings are biased and harmful

-   They are supporting efforts underway throughout the profession to change these terms, and are also taking a localized approach to replacing some harmful and racist terms with acceptable local headings in their own catalog

-   When describing some archival collections, they include a brief note to patrons alerting them to harmful or pejorative language

![](images/example.png){fig-align="center"}

::: footer
<https://www.library.virginia.edu/policies/statement-on-harmful-language>
:::

## Efforts by Libraries {.smaller}

::: r-stack
![](images/example3.png){.fragment width="550" height="350"} ![](images/example2.png){.fragment width="450" height="250"}
:::

::: footer
<https://library.virginia.edu/about-uva-library/subject-headings>

[Growing Reparative Taxonomy](https://myuva-my.sharepoint.com/:x:/g/personal/jtb4t_virginia_edu/EeGfxdWzdE1GoCjRBo4-W_YBbGhzdqWcxBdNrHf_ePbTng?rtime=KGDpdNo83Eg)

[Form Link](https://www.lib.virginia.edu/about-uva-library/subject-headings/form)
:::

## Discussion {.smaller}

Name

Name

-   Take a few minutes to discuss with a neighbor the question of your choice

## In-Class Activity {.smaller}

## Upcoming Deadlines {.smaller}

**`Spring Break: 9-17 March`**

> **Weekly Discussion Post (Week 10):** *`March 17, 2024 (Sunday) by 11:59 PM`*
>
> **Proposal topic for Final Assignment:** *`March 19, 2024 (Tuesday) by 11:59 PM`*

`Start Prepping for:`

-   *Group A-K Final Project Presentation* **(April 2)**
-   *Group L-S Final Project Presentation* **(April 9)**
-   *Professional engagement post* **(April 9)**
-   *Technical learning post* **(April 23)**

## Next Week (Mar 19): Preservation & Accessibility {.smaller}

![](images/week-9.png){fig-align="center"}

## Office Hours

Open Office Hours: **`Mondays 1-2 PM`**; **`Wednesdays 2-3 PM`**

::: columns
::: {.column width="50%"}
**Dr. Manika Lamba**

Email: ***manika\@illinois.edu***
:::

::: {.column width="50%"}
**Dr. Liliana Giusti Serra**

Email: ***lilianag\@illinois.edu***
:::
:::
